{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots to visualize data collected by ChorusWaves search script\n",
    "# Date created: not sure sorry :(\n",
    "# Last modified: 10/24/24\n",
    "# Author: Max Feinland for Blum Research Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example event plots\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sampex # thanks to Mike Shumko for making this package\n",
    "import matplotlib.dates as dates\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\" # bc I don't like the default Python font\n",
    "\n",
    "def plot_date(ax, j):\n",
    "    # Function that makes plots of events\n",
    "    # Author: Max Feinland\n",
    "    # Date created: not sure, sorry\n",
    "    # Last modified: 7/23/2024\n",
    "    \n",
    "    # Inputs: axes object (which subplot to plot on), iterator variable, \n",
    "    # dictionary containing specifics for each plot\n",
    "    \n",
    "    # Outputs: none\n",
    "    \n",
    "    b1 = 40 # buffer between start time and start of plotting (in indices)\n",
    "    b2 = 150 # buffer between stop time and end of plotting (in indices)\n",
    "    t = plotting_data[\"t\"][j] # timestamp of event start\n",
    "    \n",
    "    cur_t = pd.to_datetime(t) # convert text to DateTime\n",
    "    h = sampex.HILT(cur_t) # request SAMPEX count rate data\n",
    "    h.load()\n",
    "    \n",
    "    # find index of where SAMPEX data time matches timestamp\n",
    "    m = np.where(h.times == cur_t)[0][0]\n",
    "    d_plot = h.times[m-b1:m+b2]\n",
    "    r_plot = h.counts[m-b1:m+b2]\n",
    "    \n",
    "    maxdis = max(r_plot) - min(r_plot) # range of interval\n",
    "    # identify peaks in interval\n",
    "    [pks, _] = find_peaks(r_plot, prominence=0.25*maxdis, distance=3)\n",
    "    if j in [0]:\n",
    "        # The actual process of peaks found is a lot more finicky than this,\n",
    "        # and can be found in the ChorusWaves search script. This version runs\n",
    "        # a lot faster, albeit less accurately, so this line just fixes any discrepancy\n",
    "        # between this method and the more robust way.\n",
    "        pks = pks[0:4]\n",
    "    elif j in [2]:\n",
    "        pks = pks[2:6]\n",
    "    elif j in [4]:\n",
    "        pks = pks[1:5]\n",
    "    \n",
    "    # for plotting\n",
    "    txt_loc_x = h.times[m-b1]\n",
    "    txt_loc_y = 0.8*(max(r_plot) - min(r_plot)) + min(r_plot)\n",
    "    xlbel = \"Time (UTC) on \" + cur_t.strftime('%m/%d/%Y')\n",
    "    \n",
    "    ax.plot(d_plot, r_plot)\n",
    "    ax.plot(d_plot[pks], r_plot[pks], 'r*', markersize=10)\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%H:%M:%S')) \n",
    "    ax.set_xlabel(xlbel, fontsize=16)\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(\"Count rate (#/20ms)\", fontsize=16)\n",
    "    ax.text(x=txt_loc_x, y=txt_loc_y, s=plotting_data[\"letter\"][j], fontsize=20,\n",
    "           backgroundcolor=\"white\")\n",
    "    ax.tick_params(labelsize=12)\n",
    "    ax.set_ylim((min(r_plot)-0.1*np.ptp(r_plot)), (1.1*np.ptp(r_plot)+min(r_plot)))\n",
    "    \n",
    "# read in data\n",
    "data = pd.read_csv(\"all_events_v2.csv\", index_col=0)\n",
    "good_data = data[data.final_eye<2] # restrict to good events only\n",
    "good_data = good_data.reset_index(drop=True)\n",
    "\n",
    "idx =  [15, 25, 12, 33, 64] # indices of events I picked as examples\n",
    "plotting_data = {\"t\": [good_data.t[x] for x in idx], \n",
    "                 \"letter\": ['a) - decreasing', 'b) - crown', 'c) - flat', 'd) - half',\n",
    "                            'f) - other']}\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(20,8), constrained_layout=True)\n",
    "ax_flat = ax.flatten()\n",
    "ax_flat[4].set_visible(False)\n",
    "\n",
    "# iterate through axes object and plot each example event\n",
    "for j, a in enumerate(ax_flat):\n",
    "    if j not in [4, 5]:\n",
    "        plot_date(a, j)\n",
    "        if j == 3:\n",
    "            a.set_position([0.15, -.05, 0.3, 0.5])\n",
    "    elif j in [5]:\n",
    "        plot_date(a, j-1)\n",
    "        a.set_position([0.55, -0.05, 0.3, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MLT & L-shell histograms\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "# read in data\n",
    "data = pd.read_csv(\"all_events_v3.csv\") # my data\n",
    "ref = pd.read_csv(\"microburst_catalog_00.txt\", sep=\",\") # reference data\n",
    "pll = pd.read_csv(\"pll.csv\", index_col=0) # portion in losscone \n",
    "\n",
    "# limit reference data to time surveyed by my search script\n",
    "time_needed = pd.to_datetime(ref['dateTime'])\n",
    "idx = np.where((time_needed >= datetime(2000, 1, 1)) & (time_needed <= datetime(2003, 12, 31)))[0]\n",
    "ref = ref.iloc[idx,:]\n",
    "\n",
    "\n",
    "def make_hist(ax1, param, rmin, rmax, bin_num, losscone):\n",
    "\n",
    "\n",
    "    ax2 = ax1.twinx() # make second axis for reference data\n",
    "\n",
    "    if losscone == \"losscone\":\n",
    "        term = (pll.portion_losscone_1 < 1)\n",
    "    else:\n",
    "        term = (pll.portion_losscone_1 == 1)\n",
    "    ax1.hist(data[param][(data.final_eye<2) & term], \n",
    "             bins=bin_num, histtype='step', range=(rmin,rmax),\n",
    "             label='good bouncing packets',  color='black', linewidth=3, zorder=50)\n",
    "    ax2.hist(ref[param], bins=bin_num, range=(rmin, rmax), histtype='step', label='all microbursts', \n",
    "             color='red', linewidth=3, zorder=20, linestyle='--') # plot reference data\n",
    "\n",
    "    # tick/label stuff (formatting)\n",
    "    ax1.tick_params(labelsize=12)\n",
    "    ax1.set_xlabel(param, fontsize=14)\n",
    "    ax1.set_ylabel('Counts (#), good bouncing packets', fontsize=14)\n",
    "    ax1.grid()\n",
    "    ax1.set_xlim(rmin, rmax)\n",
    "\n",
    "    ax2.set_ylabel('Counts (#), all microbursts', color=\"lightcoral\", fontsize=14)\n",
    "    ax2.tick_params(axis='y', labelcolor=\"lightcoral\", labelsize=12)\n",
    "    ax2.set_xlim(rmin, rmax)\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(2, 1, figsize=(14,11))\n",
    "# # Call function for each variable\n",
    "make_hist(ax[0], \"MLT\", 0, 24, 24, \"losscone\")\n",
    "make_hist(ax[1], \"L\", 1, 8, 14, \"losscone\")\n",
    "\n",
    "fig1, ax1 = plt.subplots(2, 1, figsize=(14,11))\n",
    "# # Call function for each variable\n",
    "make_hist(ax1[0], \"MLT\", 0, 24, 24, \"no\")\n",
    "make_hist(ax1[1], \"L\", 1, 8, 14, \"no\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MLT & L-shell histograms\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "# read in data\n",
    "data = pd.read_csv(\"all_events_v3.csv\") # my data\n",
    "ref = pd.read_csv(\"microburst_catalog_00.txt\", sep=\",\") # reference data\n",
    "\n",
    "# limit reference data to time surveyed by my search script\n",
    "time_needed = pd.to_datetime(ref['dateTime'])\n",
    "idx = np.where((time_needed >= datetime(2000, 1, 1)) & (time_needed <= datetime(2003, 12, 31)))[0]\n",
    "ref = ref.iloc[idx,:]\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(11, 18))\n",
    "def make_hist(ax1, param, rmin, rmax, bin_num, shape):\n",
    "    # Function that makes histograms of variables of your choosing\n",
    "    # Author: Max Feinland\n",
    "    # Date created: 7/20/2024\n",
    "    # Last modified: 10/24/2024\n",
    "    \n",
    "    # Inputs: axes object (which subplot to plot on), variable name, \n",
    "    # minimum range, maximum range, number of bins\n",
    "    \n",
    "    # Outputs: none\n",
    "    \n",
    "    ax2 = ax1.twinx() # make second axis for reference data\n",
    "    \n",
    "    # plot the events\n",
    "#     ax1.hist(data[param][(data.final_eye<2) & ((data.shapes=='decr') | (data.shapes=='crown') |\n",
    "#                                               (data.shapes=='other'))], \n",
    "#              bins=bin_num, histtype='step', range=(rmin,rmax),\n",
    "#              label='good bouncing packets',  color='black', linewidth=3, zorder=50)\n",
    "    if shape == \"y\":\n",
    "    ax1.hist(data[param][(data.final_eye<2) & (data.shapes==shape)], \n",
    "             bins=bin_num, histtype='step', range=(rmin,rmax),\n",
    "             label='good bouncing packets',  color='black', linewidth=3, zorder=50)\n",
    "    ax2.hist(ref[param], bins=bin_num, range=(rmin, rmax), histtype='step', label='all microbursts', \n",
    "             color='red', linewidth=3, zorder=20, linestyle='--') # plot reference data\n",
    "    \n",
    "    # tick/label stuff (formatting)\n",
    "    ax1.tick_params(labelsize=12)\n",
    "    ax1.set_xlabel(param, fontsize=14)\n",
    "    ax1.set_ylabel('Counts (#), good bouncing packets', fontsize=14)\n",
    "    ax1.grid()\n",
    "    ax1.set_xlim(rmin, rmax)\n",
    "    \n",
    "    ax2.set_ylabel('Counts (#), all microbursts', color=\"lightcoral\", fontsize=14)\n",
    "    ax2.tick_params(axis='y', labelcolor=\"lightcoral\", labelsize=12)\n",
    "    ax2.set_xlim(rmin, rmax)\n",
    "    \n",
    "    # adding subplot label\n",
    "    if param==\"MLT\":\n",
    "        ax1.text(20, 2, shape, fontsize=20)\n",
    "        ax1.set_xlabel(\"Magnetic Local Time\", fontsize=14) # just because the variable is named\n",
    "        # \"MLT\" in the datasets, but I wanted the x-axis label to be more descriptive\n",
    "#     else:\n",
    "#         ax1.text(1.25, 40, \"b)\", fontsize=20)\n",
    "#         ax1.set_xlabel(\"L-shell\", fontsize=14) # just because the variable is named\n",
    "#         # \"L\" in the datasets, but I wanted the x-axis label to be more descriptive\n",
    "\n",
    "# # # Call function for each variable\n",
    "make_hist(ax[0], \"MLT\", 0, 24, 24, \"y\")\n",
    "make_hist(ax[1], \"MLT\", 0, 24, 24, \"n\")\n",
    "# make_hist(ax[2], \"MLT\", 0, 24, 24, \"flat\")\n",
    "# make_hist(ax[3], \"MLT\", 0, 24, 24, \"half\")\n",
    "# make_hist(ax[4], \"MLT\", 0, 24, 24, \"losscone\")\n",
    "# make_hist(ax[5], \"MLT\", 0, 24, 24, \"other\")\n",
    "# make_hist(ax[1], \"L\", 1, 8, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46822b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Period vs. L-shell\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import ast\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "def plot_specific_shape(ax, k, fullper, norm, cmap, diff, pl1):\n",
    "    # Function that plots bounce period vs. L shell for a specific shape\n",
    "    # Author: Max Feinland\n",
    "    # Date created: sometime in July 2024\n",
    "    # Last modified: 8/30/2024\n",
    "    \n",
    "    # Inputs: axes object (which subplot to plot on), iterator variable\n",
    "    \n",
    "    # Outputs: none\n",
    "    \n",
    "    current_shape = txt[\"shapes\"][k] # pull stuff out of dict\n",
    "    current_letter = txt[\"letter\"][k]\n",
    "    \n",
    "    # find indices containing allowable shapes\n",
    "    indices = np.where(good_data.shapes==current_shape)[0]\n",
    "    \n",
    "    for j in indices:\n",
    "        num_pks = len(s.dt[j])\n",
    "        hilt_uncertainty = 0.02 # time resolution of instrument\n",
    "        err = np.sqrt(good_data.spread[j]**2 + hilt_uncertainty**2) # propagate error\n",
    "\n",
    "        full_diff = abs(s.dt[j] - good_data.tb[j])\n",
    "        half_diff = abs(s.dt[j] - good_data.tb[j]/2)\n",
    "        \n",
    "        # are any of the spacings within allowable sigma? t/f (bool)\n",
    "        full_yn = any([x <= err for x in full_diff])\n",
    "        half_yn = any([x <= err for x in half_diff])\n",
    "        \n",
    "#         print(s.dt[j], err, full_diff, full_yn, half_diff, half_yn)\n",
    "        \n",
    "        # determine if half, full, or no match\n",
    "        if full_yn & half_yn:\n",
    "            # Both conditions are true, so figure out which is closer\n",
    "            if min(full_diff) < min(half_diff):\n",
    "                fullper[j] = 1\n",
    "                edgecol = full_color\n",
    "                tb = good_data.tb[j]\n",
    "                zo = 10\n",
    "                diff[j] = min(full_diff)\n",
    "            else:\n",
    "                fullper[j] = 2\n",
    "                edgecol = half_color\n",
    "                tb = good_data.tb[j]/2\n",
    "                zo = 10\n",
    "                diff[j] = min(half_diff)\n",
    "        elif full_yn:\n",
    "            fullper[j] = 1\n",
    "            edgecol = full_color\n",
    "            tb = good_data.tb[j]\n",
    "            zo = 10\n",
    "            diff[j] = min(full_diff)\n",
    "        elif half_yn:\n",
    "            fullper[j] = 2\n",
    "            edgecol = half_color\n",
    "            tb = good_data.tb[j]/2\n",
    "            zo = 10\n",
    "            diff[j] = min(half_diff)\n",
    "        else:\n",
    "            fullper[j] = 0\n",
    "            edgecol = else_color\n",
    "            zo = 5\n",
    "            if min(full_diff) < min(half_diff):\n",
    "                tb = good_data.tb[j]\n",
    "                diff[j] = min(full_diff)\n",
    "            else:\n",
    "                tb = good_data.tb[j]/2\n",
    "                diff[j] = min(half_diff)\n",
    "                \n",
    "        if pl1[j] == 1:\n",
    "            marker = '^'\n",
    "        else:\n",
    "            marker = 'o'\n",
    "        # plotting\n",
    "        handl = ax.scatter(good_data.L[j]*np.ones(len(s.dt[j])), s.dt[j], marker=marker, s=100, \n",
    "                   c=pl1[j]*np.ones(len(s.dt[j])), cmap=cmap, norm=norm, alpha = 0.8, zorder=zo,\n",
    "                          edgecolor='k')\n",
    "#         ax.plot(good_data.L[j], tb, 's', markerfacecolor='none', markeredgecolor=edgecol,\n",
    "#                markersize=10)\n",
    "        ax.errorbar(good_data.L[j], tb, yerr=err, fmt='s', \n",
    "                 markerfacecolor='none', capsize=2.5, markeredgecolor=edgecol,\n",
    "                 zorder=(zo+5), markersize=10, color='black', markeredgewidth=2, alpha=0.8)\n",
    "    \n",
    "    \n",
    "    if current_shape == 'decr':\n",
    "        shape_label = 'decreasing'\n",
    "    else:\n",
    "        shape_label = current_shape\n",
    "        \n",
    "    all_label = current_letter + ' - ' + shape_label\n",
    "    ax.text(2, 0.9, all_label, fontsize=25) # subplot label\n",
    "#     ax.text(5, 0.9, shape_label, fontsize=25)\n",
    "    ax.grid()\n",
    "    \n",
    "    if k in [3, 4, 5]: # put xlabel on bottom 4 plots\n",
    "        ax.set_xlabel(\"L-shell\", fontsize=18)\n",
    "    if k in [0, 3]: # put ylabel on leftmost 2 plots\n",
    "        ax.set_ylabel(\"Mean period (s)\", fontsize=18)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(1.5, 7)\n",
    "    ax.xaxis.set_tick_params(labelsize=15)\n",
    "    ax.yaxis.set_tick_params(labelsize=15)\n",
    "    return handl\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv(\"all_events_v3.csv\", index_col=0)\n",
    "good_data = data[data.final_eye<2] # restrict to good events\n",
    "good_data = good_data.reset_index(drop=True) # reset index\n",
    "\n",
    "# spread in model predictions\n",
    "pers = pd.read_csv(\"model_pers_2.csv\",index_col=0)\n",
    "spread = pd.DataFrame({'spread': (pers.max(axis=1) - pers.min(axis=1))})\n",
    "good_data = good_data.join(spread)\n",
    "\n",
    "# spacings between peaks for each event\n",
    "s = pd.read_csv(\"spacings.csv\")\n",
    "s['dt'] = s['dt'].apply(ast.literal_eval)\n",
    "newper = [np.min(x) for x in s.dt] # data.per is min per, use this instead\n",
    "\n",
    "# portion of particles in loss cone\n",
    "pll = pd.read_csv(\"pll.csv\", index_col=0)\n",
    "pl1 = pll.portion_losscone_1\n",
    "pl2 = pll.portion_losscone_2\n",
    "\n",
    "# Shared color normalization and colormap\n",
    "norm = mcolors.Normalize(vmin=0, vmax=1)\n",
    "cmap = plt.cm.viridis  # Use any colormap of your choice\n",
    "\n",
    "# Making figures\n",
    "fig = plt.figure(layout='constrained', figsize=(20,18))\n",
    "subfigs = fig.subfigures(2, 1, wspace=0.07)\n",
    "\n",
    "## First figure: bounce period histograms\n",
    "ax1 = subfigs[0].subplots()\n",
    "\n",
    "# Calculate period ratios for each type \n",
    "good_ratio = np.divide(newper, good_data.tb)\n",
    "# ok_ratio = np.divide(data.per[(data.final_eye>1.5) & (data.final_eye<3)], \n",
    "#                      data.tb[(data.final_eye>1.5) & (data.final_eye<3)])\n",
    "bad_ratio = np.divide(data.per[data.final_eye>1.5], data.tb[data.final_eye>1.5])\n",
    "\n",
    "ax1.hist(good_ratio, bins=15, range=(-0.05,1.45), color='royalblue', histtype='step', linewidth=4,\n",
    "        linestyle='-', zorder=10, label='good') # good histogram\n",
    "# ax1.hist(ok_ratio, bins=15, range=(-0.05,1.45), color='goldenrod', histtype='step', linewidth=4,\n",
    "#          linestyle='-.', zorder=5, label='okay') # ok histogram\n",
    "ax1.hist(bad_ratio, bins=15, range=(-0.05,1.45), color='dimgrey', histtype='step', linewidth=4,\n",
    "         linestyle='--', zorder=0, label='bad') # bad histogram\n",
    "\n",
    "# axis labels & ticks\n",
    "ax1.set_xlabel(\"Ratio of observed period to predicted period\", fontsize=20)\n",
    "ax1.set_ylabel(\"Number of observations\", fontsize=20)\n",
    "ax1.xaxis.set_tick_params(labelsize=20)\n",
    "ax1.yaxis.set_tick_params(labelsize=20)\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.grid()\n",
    "ax1.set_xlim(0, 1.5)\n",
    "ax1.set_ylim(0, 80)\n",
    "ax1.text(0.05, 75, 'a)', fontsize=30) # subplot label\n",
    "\n",
    "# Second figure: bounce period vs. L for each shape\n",
    "ax2 = subfigs[1].subplots(2, 3, sharex=True, sharey=True)\n",
    "ax_flat = ax2.flatten()\n",
    "ax_flat[4].set_visible(False)\n",
    "\n",
    "txt = {\"shapes\": [\"decr\", \"crown\", \"flat\", \"half\", \"other\"],\n",
    "      \"letter\": [\"b)\", \"c)\", \"d)\", \"e)\", \"f)\"]}\n",
    "\n",
    "full_color = 'dodgerblue'\n",
    "half_color = 'limegreen'\n",
    "else_color = 'rosybrown'\n",
    "\n",
    "fullper = np.zeros(len(good_data)) # initialize vector containing full/half classification\n",
    "diff = np.zeros(len(good_data)) # initialize vector containing difference bw expected & observed\n",
    "norm = mcolors.Normalize(vmin=0, vmax=1)\n",
    "# iterate through axes object and plot each shape type\n",
    "for j, a in enumerate(ax_flat):\n",
    "    if j < 4:\n",
    "        handle = plot_specific_shape(a, j, fullper, norm, cmap, diff, pll.portion_losscone_1)\n",
    "        if j == 3:\n",
    "            a.set_position([0.1, -.05, 0.45, 0.5])\n",
    "    elif j == 5:\n",
    "        a.set_position([0.6, -0.05, 0.45, 0.5])\n",
    "        handle = plot_specific_shape(a, j-1, fullper, norm, cmap, diff, pll.portion_losscone_1)\n",
    "cbar = fig.colorbar(handle, ax=ax_flat, orientation='vertical')\n",
    "cbar.set_label(\"Portion in losscone if isotropic\", fontsize=16)\n",
    "\n",
    "\n",
    "p = ax2[0,2]\n",
    "# p1, = p.plot(0, 0, 'o', color=full_color, alpha=0.8, markersize=10)\n",
    "# p2, = p.plot(0, 0, 'o', color=half_color, alpha=0.8, markersize=10)\n",
    "# p3, = p.plot(0, 0, 'o', color=else_color, alpha=0.8, markersize=10)\n",
    "f = p.errorbar(0, 0, yerr=0.2, fmt='s', markeredgecolor=full_color, markerfacecolor='none',\n",
    "               color='black', markersize=10, capsize=4, markeredgewidth=2)\n",
    "h = p.errorbar(0, 0, yerr=0.2, fmt='s', markeredgecolor=half_color, markerfacecolor='none',\n",
    "               color='black', markersize=10, capsize=4, markeredgewidth=2)\n",
    "x = p.errorbar(0, 0, yerr=0.2, fmt='s', markeredgecolor=else_color, markerfacecolor='none',\n",
    "               color='black', markersize=10, capsize=4, markeredgewidth=2)\n",
    "# l = p.legend([(p1, p2, p3), f, h, x], # i made the legend look nice\n",
    "#          ['Observed spacing', 'T05 model', 'T05 model (half)', 'T05 model (no match)'], \n",
    "#          handler_map={tuple: HandlerTuple(ndivide=None)}, fontsize=16) \n",
    "l = p.legend([handle, f, h, x], # i made the legend look nice\n",
    "         ['Observed spacing', 'T05 model', 'T05 model (half)', 'T05 model (no match)'], \n",
    "             fontsize=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23019d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Make polar plot of microburst distribution and compare to Hamdans paper'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Geographical location'''\n",
    "def figure_4():\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    import ast\n",
    "\n",
    "    plt.rcParams[\"font.family\"] = \"Arial\" # because I don't like the default Python font\n",
    "    \n",
    "    def determine_match(full_diff, half_diff, full_yn, half_yn):  \n",
    "        if full_yn and half_yn: # if both half & full-per matches\n",
    "            if min(full_diff) < min(half_diff):\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "        elif full_yn: # if full per match only\n",
    "            return 1\n",
    "        elif half_yn: # if half per match only\n",
    "            return 2\n",
    "        else: # no match\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def make_losscone_map(ax):\n",
    "        # Function that makes world outline and L-shell contours\n",
    "        # Inputs: axes object (which subplot to plot on)\n",
    "\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "        world.plot(ax=ax, color='none', edgecolor='black', zorder=40)\n",
    "\n",
    "        # read in SAMPEX attitude data. If you don't have this file you can download it from here:\n",
    "        # https://izw1.caltech.edu/sampex/DataCenter/DATA/PSSet/Text/PSSet_6sec_2000022_2000048.txt\n",
    "        # Make sure to change the pathing so it works with your machine.\n",
    "        att = pd.read_csv('C:/Users/maxim/sampex-data/Attitude/PSSet_6sec_2000022_2000048.txt', \n",
    "                         sep=' ', header=60, on_bad_lines='skip')\n",
    "\n",
    "        cols = np.array([7, 8, 35]) # take out lat, lon, losscone 2\n",
    "        att = att.iloc[:150000, cols] # don't need the whole thing, just about 14 days for full coverage\n",
    "        att.columns = ['lon', 'lat', 'losscone']\n",
    "\n",
    "        # change longitude to be -180 to 180\n",
    "        long_idx = np.where(att.lon > 180)[0]\n",
    "        att.lon[long_idx] = att.lon[long_idx] - 360\n",
    "\n",
    "        sc = ax.scatter(att.lon, att.lat, c=att.losscone, s=25, vmin=30, vmax=90, zorder=5, cmap='gray')\n",
    "        c = plt.colorbar(sc)\n",
    "        c.set_label(\"Losscone angle (deg)\", fontsize=16)\n",
    "\n",
    "        lgrid = pd.read_csv('Lgrid.dat', delimiter='\\t', header=None)\n",
    "\n",
    "        for i in np.arange(1, 30, 2):\n",
    "            min_pos = np.argmin(lgrid.iloc[:,i])\n",
    "            latl = np.concatenate(([lgrid.iloc[min_pos:,i-1], lgrid.iloc[:min_pos,i-1]]))\n",
    "            lonl = np.concatenate(([lgrid.iloc[min_pos:,i], lgrid.iloc[:min_pos,i]]))\n",
    "            ax.plot(lonl, latl, '--', color=\"white\", zorder=45)\n",
    "\n",
    "        ax.text(-180, 7, \"Magnetic Equator\", fontsize=16, zorder=50)\n",
    "        ax.text(-197, 48, \"L = 2\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, -40, \"L = 2\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, 59, \"L = 3\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, -55, \"L = 3\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, 64, \"L = 5\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, -60, \"L = 5\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, 71, \"L = 8\", fontsize=14, zorder=50)\n",
    "        ax.text(-197, -66, \"L = 8\", fontsize=14, zorder=50)\n",
    "\n",
    "        ax.set_xlabel(\"Longitude\", fontsize=20)\n",
    "        ax.set_ylabel(\"Latitude\", fontsize=20)\n",
    "        ax.set_ylim(-85, 85)\n",
    "        return ax\n",
    "\n",
    "    # Import data\n",
    "    data = pd.read_csv(\"all_events_v2.csv\", index_col=0)\n",
    "    good_data = data[data.final_eye<2] # restrict to good events\n",
    "    good_data = good_data.reset_index(drop=True) # reset index\n",
    "\n",
    "    # spread in model predictions\n",
    "    pers = pd.read_csv(\"model_preds_and_spacing.csv\",index_col=0)\n",
    "    models  = pers[['T89', 'T05', 'OP', 'SL']]\n",
    "    pers['spread'] = models.max(axis=1) - models.min(axis=1)\n",
    "\n",
    "    # spacings between peaks for each event\n",
    "    pers['dt'] = pers['dt'].apply(ast.literal_eval)\n",
    "    newper = [np.mean(x) for x in pers.dt] # calculate new mean spacing\n",
    "\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(18, 14))\n",
    "\n",
    "    # call losscone map function\n",
    "    ax1 = make_losscone_map(ax1)\n",
    "    ax2 = make_losscone_map(ax2)\n",
    "\n",
    "    fullper = np.zeros(len(good_data)) # initialize vector containing full/half classification\n",
    "\n",
    "    # create full/half classification\n",
    "    # this is kept dynamic so you can change the error condition if you need to\n",
    "    for j in range(len(good_data.tb)):\n",
    "            hilt_uncertainty = 0.02 # time resolution of instrument\n",
    "            err = np.sqrt(pers.spread[j]**2 + hilt_uncertainty**2) # propagate error\n",
    "\n",
    "            fulldiff = abs(pers.dt[j] - good_data.tb[j])\n",
    "            halfdiff = abs(pers.dt[j] - good_data.tb[j]/2)\n",
    "\n",
    "            # are any of the spacings within allowable error bounds? \n",
    "            fullyn = any([x <= err for x in fulldiff])\n",
    "            halfyn = any([x <= err for x in halfdiff])\n",
    "            \n",
    "            fullper[j] = determine_match(fulldiff, halfdiff, fullyn, halfyn)\n",
    "\n",
    "    # subplot 1: half/full/fail\n",
    "    full = ax1.scatter(good_data.lon[fullper==1], good_data.lat[fullper==1], \n",
    "                      s=100,  c='dodgerblue', label='full period', zorder=55, edgecolor='navy')\n",
    "    half = ax1.scatter(good_data.lon[fullper==2], good_data.lat[fullper==2], \n",
    "                      s=100,  c='limegreen', label='half period', zorder=50, edgecolor='darkgreen')\n",
    "    fail = ax1.scatter(good_data.lon[fullper==0], good_data.lat[fullper==0], \n",
    "                      s=100,  c='rosybrown', label='no match', zorder=45, edgecolor='maroon')\n",
    "    ax1.legend(loc='right', fontsize=14).set_zorder(50)\n",
    "\n",
    "    # subplot 2: shapes\n",
    "    decr = ax2.scatter(good_data.lon[good_data.shapes==\"decr\"], good_data.lat[good_data.shapes==\"decr\"], \n",
    "                      s=100,  c='royalblue', label='decreasing', zorder=50, edgecolor='navy')\n",
    "    blake = ax2.scatter(good_data.lon[good_data.shapes==\"blake\"], good_data.lat[good_data.shapes==\"blake\"],\n",
    "                       s=100, c='mediumslateblue',  label='blake', zorder=50, edgecolor='rebeccapurple')\n",
    "    crown = ax2.scatter(good_data.lon[good_data.shapes==\"crown\"], good_data.lat[good_data.shapes==\"crown\"],\n",
    "                       s=100, c='fuchsia', label='crown', zorder=50, edgecolor='purple')\n",
    "    flat = ax2.scatter(good_data.lon[good_data.shapes==\"flat\"], good_data.lat[good_data.shapes==\"flat\"],\n",
    "                       s=100, c='turquoise', label='flat', zorder=50, edgecolor='darkslategray')\n",
    "    incr = ax2.scatter(good_data.lon[good_data.shapes==\"incr\"], good_data.lat[good_data.shapes==\"incr\"],\n",
    "                       s=100, c='orangered', label='increasing', zorder=50, edgecolor='maroon')\n",
    "    smile = ax2.scatter(good_data.lon[good_data.shapes==\"smile\"], good_data.lat[good_data.shapes==\"smile\"],\n",
    "                       s=100, c='purple', label='smile', zorder=50, edgecolor='darkmagenta')\n",
    "    half = ax2.scatter(good_data.lon[good_data.shapes==\"half\"], good_data.lat[good_data.shapes==\"half\"],\n",
    "                       s=100, c='goldenrod', label='half', zorder=50, edgecolor='darkgoldenrod')\n",
    "    other = ax2.scatter(good_data.lon[good_data.shapes==\"other\"], good_data.lat[good_data.shapes==\"other\"],\n",
    "                       s=100, c='lawngreen', label='other', zorder=50, edgecolor='olivedrab')\n",
    "\n",
    "    ax2.legend(loc='right', fontsize=14).set_zorder(50)\n",
    "figure_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185565b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20616018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isleap(year):\n",
    "    return year % 4 == 0\n",
    "\n",
    "def dayarrayfun():\n",
    "    \"\"\"Generate a DataFrame of day and year values.\"\"\"\n",
    "    start_year = 1996\n",
    "    start_day = 160\n",
    "    end_year = 2012\n",
    "    end_day = 283\n",
    "    spacing = 27\n",
    "\n",
    "    days_array = []\n",
    "    years_array = []\n",
    "\n",
    "    current_year = start_year\n",
    "    current_day = start_day\n",
    "\n",
    "    while current_year < end_year or (current_year == end_year and current_day <= end_day):\n",
    "        # Determine the number of days in the current year\n",
    "        num_days = 366 if isleap(current_year) else 365\n",
    "        \n",
    "        # Append the current day and year to the arrays\n",
    "        days_array.append(current_day)\n",
    "        years_array.append(current_year)\n",
    "        \n",
    "        # Increment the current day by the spacing\n",
    "        current_day += spacing\n",
    "        if current_day > num_days:\n",
    "            current_day -= num_days\n",
    "            current_year += 1\n",
    "\n",
    "    y = [str(x) + str(y) for x, y in zip(years_array, days_array)]\n",
    "    dates = pd.to_datetime(y, format='%Y%j')\n",
    "    days_df = pd.DataFrame({'dates': dates, 'y_col': years_array, 'd_col': days_array})\n",
    "    return days_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_att(t):\n",
    "    days_df = dayarrayfun()\n",
    "    pathname = 'C:/Users/maxim/sampex-data/Attitude/'\n",
    "    # find the right row in the ephemeris files\n",
    "    att_row = np.where(days_df['dates'] < t)[0][-1]\n",
    "    day1str = str(days_df['d_col'][att_row])\n",
    "    day2str = str(days_df['d_col'][att_row+1]-1)\n",
    "    \n",
    "    # checking if leading zeroes are needed\n",
    "    if len(day1str) == 1:\n",
    "        day1str = '00' + day1str\n",
    "    elif len(day1str) == 2:\n",
    "        day1str = '0' + day1str\n",
    "\n",
    "    if len(day2str) == 1:\n",
    "        day2str = '00' + day2str\n",
    "    elif len(day2str) == 2:\n",
    "        day2str = '0' + day2str\n",
    "    filename = 'PSSet_6sec_' + str(days_df['y_col'][att_row]) + day1str + '_' + \\\n",
    "                str(days_df['y_col'][att_row+1]) + day2str + '.txt'\n",
    "    try:\n",
    "        att = pd.read_csv(pathname + filename, sep=' ', header=60, on_bad_lines='skip')\n",
    "    except:\n",
    "        a = sampex.Attitude(t) # download da file\n",
    "        att = pd.read_csv(pathname + filename, sep=' ', header=60, on_bad_lines='skip')\n",
    "\n",
    "    cols = np.array([0, 1, 2, 7, 8, 34, 35, 68]) # take out lat, lon, losscone 2\n",
    "    att = att.iloc[:, cols] \n",
    "    att.columns = ['year', 'doy', 'second', 'lon', 'lat', 'losscone1', 'losscone2', 'pitch']\n",
    "\n",
    "    # change longitude to be -180 to 180\n",
    "    long_idx = np.where(att.lon > 180)[0]\n",
    "    att.lon[long_idx] = att.lon[long_idx] - 360\n",
    "\n",
    "    year = np.array(att.iloc[:, 0])\n",
    "    doy = np.array(att.iloc[:, 1])\n",
    "    second = np.array(att.iloc[:, 2])\n",
    "    \n",
    "    def calc_t(year, doy, s):\n",
    "        start_of_year = datetime(int(year), 1, 1)\n",
    "        return start_of_year + timedelta(days=int(doy) - 1, seconds=int(s))\n",
    "\n",
    "    vectorized_datetime = np.vectorize(calc_t)\n",
    "    att_t = vectorized_datetime(year, doy, second)\n",
    "    return att_t, att\n",
    "\n",
    "def interpolate_pitch_and_losscone(att_t, att, a_idx, t):\n",
    "    l = []\n",
    "    t1 = att_t[a_idx-1] # time 1 to interpolate with\n",
    "    t2 = att_t[a_idx] # time 2, after desired timestamp\n",
    "    dt = (t2 - t1).total_seconds() # change in time\n",
    "    att_keys = ['pitch', 'losscone1', 'losscone2'] # values you want to interpolate\n",
    "    for k in att_keys:\n",
    "        dy = att[k][a_idx] - att[k][a_idx-1] # change in y for whichever key you're on\n",
    "        slope = dy/dt # change in y over change in time\n",
    "        fval = att[k][a_idx-1] + slope * (t - t1).total_seconds() # interpolation at time t\n",
    "        l.append(fval)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e11354",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Comparing pitch angle w/ losscone angle'''\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from IRBEM import MagFields\n",
    "att_already_loaded = None\n",
    "print_yn = True\n",
    "x = []\n",
    "\n",
    "days_df = dayarrayfun() # call list of year-doy pairs\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv(\"all_events_v2.csv\", index_col=0)\n",
    "good_data = data[data.final_eye<2] # restrict to good events\n",
    "good_data = good_data.reset_index(drop=True) # reset index\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    t = datetime.strptime(data['t'][i], '%Y-%m-%d %H:%M:%S.%f') # convert from string to datetime\n",
    "    att_row = np.where(days_df['dates'] < t)[0][-1] \n",
    "    \n",
    "    # Check to see if attitude loading needed\n",
    "    if 'old_row' in globals():\n",
    "        if att_row != old_row:\n",
    "            att_already_loaded = False\n",
    "        else:\n",
    "            att_already_loaded = True\n",
    "        \n",
    "    # Load attitude data if needed\n",
    "    if att_already_loaded is not True:\n",
    "        if print_yn == True:\n",
    "            print(f\"\\nLoading attitude data...\")\n",
    "        att_t, att = load_att(t)\n",
    "        old_row = att_row\n",
    "\n",
    "    a_idx = np.where(att_t > t)[0][0] # first time attitude time data is after desired timestamp\n",
    "    # Interpolate values\n",
    "    pll = interpolate_pitch_and_losscone(att_t, att, a_idx, t) # pitch, losscone1, losscone2\n",
    "    x.append(pll)\n",
    "    print(x)\n",
    "    \n",
    "p = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "for i in range(len(x)):\n",
    "    p.append(x[i][0])\n",
    "    l1.append(x[i][1])\n",
    "    l2.append(x[i][2])\n",
    "\n",
    "\n",
    "# LLA = {'x1':att.iloc[row,9], 'x2':att.iloc[row,7], 'x3':att.iloc[row,8], 'dateTime':datestring}\n",
    "# maginput = {'Kp':40}\n",
    "# output_dictionary = model.make_lstar_shell_splitting(LLA, alpha, maginput)\n",
    "# print(output_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Geographical location'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import ast\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\" # because I don't like the default Python font\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 14))\n",
    "ax.set_facecolor('deepskyblue')\n",
    "shapefile = 'ne_110m_admin_0_countries_lakes.shp'\n",
    "\n",
    "world = gpd.read_file(shapefile)\n",
    "world.plot(ax=ax, color='springgreen', edgecolor='black', zorder=40)\n",
    "\n",
    "att = pd.read_csv('C:/Users/maxim/sampex-data/Attitude/PSSet_6sec_2000022_2000048.txt', \n",
    "                 sep=' ', header=60, on_bad_lines='skip')\n",
    "\n",
    "cols = np.array([7, 8, 35]) # take out lat, lon, losscone 2\n",
    "att = att.iloc[:150000, cols] # don't need the whole thing, just about 14 days for full coverage\n",
    "att.columns = ['lon', 'lat', 'losscone']\n",
    "\n",
    "\n",
    "lgrid = pd.read_csv('Lgrid.dat', delimiter='\\t', header=None)\n",
    "\n",
    "for i in np.arange(1, 30, 2):\n",
    "    min_pos = np.argmin(lgrid.iloc[:,i])\n",
    "    latl = np.concatenate(([lgrid.iloc[min_pos:,i-1], lgrid.iloc[:min_pos,i-1]]))\n",
    "    lonl = np.concatenate(([lgrid.iloc[min_pos:,i], lgrid.iloc[:min_pos,i]]))\n",
    "    ax.plot(lonl, latl, '--', color=\"white\", zorder=45)\n",
    "\n",
    "ax.text(-180, 7, \"Magnetic Equator\", fontsize=16, zorder=50)\n",
    "ax.text(-197, 48, \"L = 2\", fontsize=14, zorder=50)\n",
    "ax.text(-197, -40, \"L = 2\", fontsize=14, zorder=50)\n",
    "ax.text(-197, 59, \"L = 3\", fontsize=14, zorder=50)\n",
    "ax.text(-197, -55, \"L = 3\", fontsize=14, zorder=50)\n",
    "ax.text(-197, 64, \"L = 5\", fontsize=14, zorder=50)\n",
    "ax.text(-197, -60, \"L = 5\", fontsize=14, zorder=50)\n",
    "ax.text(-197, 71, \"L = 8\", fontsize=14, zorder=50)\n",
    "ax.text(-197, -66, \"L = 8\", fontsize=14, zorder=50)\n",
    "\n",
    "ax.set_xlabel(\"Longitude\", fontsize=20)\n",
    "ax.set_ylabel(\"Latitude\", fontsize=20)\n",
    "ax.set_ylim(-85, 85)\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv(\"all_events_v2.csv\", index_col=0)\n",
    "good_data = data[data.final_eye<2] # restrict to good events\n",
    "good_data = good_data.reset_index(drop=True) # reset index\n",
    "\n",
    "ax.plot(good_data.lon[0], good_data.lat[0], '*', markersize=20, color='red', zorder=50)\n",
    "print(pll_df.p[0], pll_df.l1[0], pll_df.l2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45baee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# really_good_days = np.where(good_data.final_eye==1)[0]\n",
    "pll_df = pd.read_csv('pll.csv', index_col=0)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    f, a = plt.subplots(figsize=(7,7))\n",
    "\n",
    "    # coordinate axes\n",
    "    a.plot(np.linspace(-1.5, 1.5, 20), np.zeros(20), 'k-', zorder=0)\n",
    "    a.plot(np.zeros(20), np.linspace(-1.5, 1.5, 20), 'k-', zorder=0)\n",
    "\n",
    "    # pitch angle\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.p[i])), np.sin(np.deg2rad(90-pll_df.p[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='r', label='Pitch')\n",
    "    print('pitch angle:', pll_df.p[i])\n",
    "\n",
    "    # either side due to FOV\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.p[i] + 34)), np.sin(np.deg2rad(90-pll_df.p[i] + 34)), \n",
    "             angles='xy', scale_units='xy', scale=1, color='salmon', label='Pitch at edge of FOV', \n",
    "             width= 0.002)\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.p[i] - 34)), np.sin(np.deg2rad(90-pll_df.p[i] - 34)), \n",
    "             angles='xy', scale_units='xy', scale=1, color='salmon', label=None, width= 0.002)\n",
    "\n",
    "    # loss cone 1\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.l1[i])), np.sin(np.deg2rad(90-pll_df.l1[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='orange', \n",
    "             label='Loss cone (same hemisphere)')\n",
    "    a.quiver(0, 0, -np.cos(np.deg2rad(90-pll_df.l1[i])), np.sin(np.deg2rad(90-pll_df.l1[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='orange')\n",
    "    # southern hemisphere\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.l1[i])), -np.sin(np.deg2rad(90-pll_df.l1[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='orange')\n",
    "    a.quiver(0, 0, -np.cos(np.deg2rad(90-pll_df.l1[i])), -np.sin(np.deg2rad(90-pll_df.l1[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='orange')\n",
    "    \n",
    "    # loss cone 2\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.l1[i])), np.sin(np.deg2rad(90-pll_df.l2[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='gold', \n",
    "             label='Loss cone (either hemisphere)')\n",
    "    a.quiver(0, 0, -np.cos(np.deg2rad(90-pll_df.l1[i])), np.sin(np.deg2rad(90-pll_df.l2[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='gold')\n",
    "    # southern hemisphere\n",
    "    a.quiver(0, 0, np.cos(np.deg2rad(90-pll_df.l1[i])), -np.sin(np.deg2rad(90-pll_df.l2[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='gold')\n",
    "    a.quiver(0, 0, -np.cos(np.deg2rad(90-pll_df.l1[i])), -np.sin(np.deg2rad(90-pll_df.l2[i])), \n",
    "             angles='xy', scale_units='xy', scale=1, color='gold')\n",
    "    \n",
    "    # magnetic field\n",
    "    a.quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='b', label='-B-field')\n",
    "    a.quiver(0, 0, 0, -1, angles='xy', scale_units='xy', scale=1, color='navy')\n",
    "    \n",
    "    a.text(-1.45, 1.4, str(good_data.t[i]))\n",
    "    a.text(-1.45, 1.3, 'Pitch angle: ' + str(round(pll_df.p[i], 2)) + '$^{\\circ}$')\n",
    "    a.text(-1.45, 1.2, 'Loss cone 1: ' + str(round(pll_df.l1[i],2)) + '$^{\\circ}$')\n",
    "    a.text(-1.45, 1.1, 'Loss cone 2: ' + str(round(pll_df.l2[i], 2)) + '$^{\\circ}$')\n",
    "\n",
    "    a.set_xlim(-1.5, 1.5)\n",
    "    a.set_ylim(-1.5,1.5)\n",
    "    a.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Determine each % of particles observed within loss cone.\n",
    "Mathematically, I think I can just say that the angle is 1/360th of the total, right?'''\n",
    "# So, the portion of particles in the loss cone is the angle from the outermost edge of pitch at \n",
    "# edge of FOV to the \n",
    "# over 68deg always.\n",
    "# prop1 = []\n",
    "prop2 = []\n",
    "for i in range(len(pll_df)):\n",
    "#     print(i, good_data.t[i])\n",
    "    pitch_angle = pll_df.p[i]\n",
    "#     loss_cone_1 = pll_df.l1[i]\n",
    "    loss_cone_1 = pll_df.l2[i]\n",
    "    pitch_upper_limit = pitch_angle + 34\n",
    "    pitch_lower_limit = pitch_angle - 34\n",
    "#     print('pitch_upper_limit', round(pitch_upper_limit,2), 'pitch_lower_limit', \n",
    "#           round(pitch_lower_limit, 2), 'loss_cone_1', round(loss_cone_1,2))\n",
    "\n",
    "    if loss_cone_1 == 90:\n",
    "        portion_in_loss_cone1 = 1 #everything in loss cone\n",
    "    elif (pitch_lower_limit < 90) and (pitch_upper_limit < (180-loss_cone_1)) :\n",
    "        if (pitch_lower_limit < 0) and (pitch_upper_limit < loss_cone_1):\n",
    "            portion_in_loss_cone1 = 1 #everything in loss cone\n",
    "        elif loss_cone_1 > pitch_lower_limit:\n",
    "            portion_in_loss_cone1 = (loss_cone_1 - pitch_lower_limit)/68\n",
    "    elif (pitch_upper_limit > 90) and (pitch_lower_limit > loss_cone_1):\n",
    "        if (pitch_upper_limit > (180-loss_cone_1)) and (pitch_lower_limit > (180-loss_cone_1)):\n",
    "            portion_in_loss_cone1 = 1 #everything in loss cone\n",
    "        else:\n",
    "            portion_in_loss_cone1 = (pitch_upper_limit - (180-loss_cone_1))/68\n",
    "    elif (pitch_lower_limit < 90) and (loss_cone_1 > pitch_lower_limit) and \\\n",
    "    (pitch_upper_limit > (180-loss_cone_1)):\n",
    "        portion_in_loss_cone1 = ((pitch_upper_limit - (180-loss_cone_1)) + \n",
    "                                (loss_cone_1 - pitch_lower_limit))/68\n",
    "    else:\n",
    "#         print(' fuck')\n",
    "        break\n",
    "#     print(portion_in_loss_cone1)\n",
    "    if (portion_in_loss_cone1 > 1) or (portion_in_loss_cone1 < 0):\n",
    "#         print('bad')\n",
    "        break\n",
    "    prop2.append(portion_in_loss_cone1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'portion_losscone_1': prop1, 'portion_losscone_2': prop2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04afb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pll_df = pd.read_csv('pll.csv', index_col=0)\n",
    "final_df = pll_df.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df)\n",
    "final_df.to_csv('pll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd2b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
