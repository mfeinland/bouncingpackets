{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bouncing Packet Locator Script\n",
    "\n",
    "# Date Created: 05/15/23 (Original Script, in MATLAB)\n",
    "# Date Created: 01/03/24 (This Script)\n",
    "# Last Modified: 01/29/24\n",
    "\n",
    "# Author: Max Feinland for Blum Research Group, LASP\n",
    "\n",
    "# Inputs: start date, end date\n",
    "\n",
    "# Description: complete driver script. The user will be prompted to specify the dates they would like to see. \n",
    "# This script will then print out and plot all of the microbursts found in the specified time period.\n",
    "\n",
    "# housekeeping\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacepy.datamodel\n",
    "import spacepy.time as spt\n",
    "# import spacepy.omni as om\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil.parser\n",
    "from IRBEM import MagFields\n",
    "import sampex\n",
    "import matplotlib.pylab as plt\n",
    "# import matplotlib.patches as patches\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import scipy.io\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## O'Brien\n",
    "\n",
    "# Date created: 3/21/23 (in MATLAB)\n",
    "# or 01/04/24 in Python\n",
    "# Last modified: 01/06/24\n",
    "# Author: Max Feinland for Blum Research Group, LASP\n",
    "\n",
    "# Inputs: SSD & time data, day\n",
    "\n",
    "# Outputs: starting & ending time of each microburst, plus time intervals\n",
    "\n",
    "def obrien(data):\n",
    "    N20 = data['counts'] # count rate sampled every 20 ms\n",
    "    time20 = data['time'] # time every 20 ms\n",
    "\n",
    "    # I'm sure there's a more efficient way to do this, but I don't know it\n",
    "    df = pd.DataFrame({'time': data['time'], 'counts': data['counts']})\n",
    "\n",
    "    df.set_index('time', inplace=True) # set time column as the index\n",
    "\n",
    "    # resample the dataframe to 100 ms intervals and sum the counts in each interval\n",
    "    N100 = df.resample('100ms').sum()\n",
    "\n",
    "    A500 = N100.rolling(5, center=True).mean() # 5-observation centered rolling mean (over 500 ms)\n",
    "\n",
    "    condition = np.divide((N100.counts - A500.counts), np.sqrt(1 + A500.counts)) # O'Brien et al 2003\n",
    "    \n",
    "    ns = np.argwhere(condition > 10)\n",
    "    ns = [item[0] for item in ns]\n",
    "\n",
    "    epsilon = 10; # if two flagged indices are spaced less than this distance apart, \n",
    "    # they are probably part of the same microburst\n",
    "\n",
    "    # initializing\n",
    "    starts = []\n",
    "    ends = []\n",
    "\n",
    "    dn = np.diff(ns) # difference in time between instances of the condition being true\n",
    "\n",
    "    # finding extended periods of the condition being true\n",
    "    for i in np.arange(1,len(dn)-10):\n",
    "        if dn[i] < epsilon and dn[i+1] < epsilon and dn[i-1] >= epsilon: # start condition\n",
    "            starts.append(ns[i])\n",
    "            for j in np.arange(i+1, len(dn)-1):\n",
    "                if dn[j] < epsilon and dn[j+1] >= epsilon:\n",
    "                    ends.append(ns[j]) # end condition\n",
    "                    break\n",
    "        elif dn[i] <= epsilon and i == 2: # start condition (edge case)\n",
    "            starts.append(ns[i])\n",
    "            for j in np.arange(i+1, len(dn-1)):\n",
    "                if dn[j] <= epsilon:\n",
    "                    ends.append(ns[j])\n",
    "                    break\n",
    "        elif i == len(dn): # end condition (edge case)\n",
    "            ends.append(ns[i])\n",
    "\n",
    "    if len(starts) > len(ends):\n",
    "        ends.append(starts[len(starts)-1] + 50)\n",
    "        \n",
    "    starts = [x - 2 for x in starts] # pad with 0.2 seconds \n",
    "    ends = [x + 10 for x in ends] # pad with 1 second\n",
    "\n",
    "\n",
    "    def changeCadence(time20, times100):\n",
    "        # times100 is the list of timestamps for whatever it is: starts, ends, ns\n",
    "        # times20 is the entire list of time20 for the day in question\n",
    "        try:\n",
    "            idx20 = [time20.get_loc(tstmp) for tstmp in times100] # try list comprehension first, it's faster\n",
    "        except: \n",
    "            idx20 = [] # clear and start over\n",
    "            idx20 = [np.abs((time20 - target).values).argmin() for target in times100] # if a timestamp is missing from time20,\n",
    "            # you will have to find the minimum timestamp (that is closest in time)\n",
    "        return idx20\n",
    "\n",
    "    # reverting indices to 20 ms cadence\n",
    "\n",
    "    starts20 = changeCadence(time20, N100.index[starts])\n",
    "    ends20 = changeCadence(time20, N100.index[ends])\n",
    "    ns20 = changeCadence(time20, N100.index[ns])\n",
    "\n",
    "    # yay, your output is ready!\n",
    "    d = dict(); \n",
    "    d['st'] = starts20\n",
    "    d['et'] = ends20\n",
    "    d['ns'] = ns20\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bouncingPackets(so, eo, h, a):\n",
    "    pksi = []\n",
    "    st = []\n",
    "    et = []\n",
    "    \n",
    "    per = []\n",
    "    L = []\n",
    "    alpha0 = []\n",
    "    X = []\n",
    "    \n",
    "    t = h.times\n",
    "    rate = h.counts\n",
    "    \n",
    "    for i in range(len(so)):\n",
    "#         print('Iteration', i)\n",
    "        interval = rate[so[i]:eo[i]] # define just one chunk of the rate data\n",
    "        maxdis = max(interval) - min(interval)\n",
    "        \n",
    "        # finding peaks with a high enough prominence\n",
    "        [pks, _] = find_peaks(interval, prominence=0.25*maxdis, distance=3)\n",
    "        npks = list(so[i] + loc for loc in pks)\n",
    "        pksi.extend(npks)\n",
    "        \n",
    "        loc = pd.to_datetime(t[npks])\n",
    "        \n",
    "        dloc = [loc[i + 1] - loc[i] for i in range(len(loc) - 1)]\n",
    "        dloc = [x.total_seconds() for x in dloc]\n",
    "        ddloc = np.diff(dloc)\n",
    "        \n",
    "        # Schulz and Lanzerotti Bounce period equation (stolen from Mike)\n",
    "        Tsl = lambda L, alpha0, v: 4*6.371E6*np.divide(L, v) * \\\n",
    "               (1.3802 - 0.3198*(np.sin(np.deg2rad(alpha0)) + \\\n",
    "               np.sqrt(np.sin(np.deg2rad(alpha0)))))\n",
    "        beta = lambda Ek: np.sqrt(1-((Ek/511)+1)**(-2)) # Lorentz factor, I think\n",
    "        c = 3.0E8 # m/s; true for the observable universe, presumably\n",
    "        KE = 1000 # keV; true of all particles observed at SAMPEX\n",
    "        \n",
    "        # att_idx = np.where(t[so[i]] >= attData.times)[0][-1]\n",
    "        \n",
    "        def tsyganenko05(attData, tstmp):\n",
    "            omniLoc = 'C:/Users/maxim/.spacepy/data/omnidata.h5'\n",
    "            omniData = spacepy.datamodel.fromHDF5(omniLoc)\n",
    "            omniT = np.array([dateutil.parser.parse(i.decode()) for i in omniData['UTC']])\n",
    "            idx = np.where(tstmp >= omniT)[0][-1]\n",
    "            T05Keys = ['Dst', 'Pdyn', 'ByIMF', 'BzIMF', 'W1', 'W2', 'W3', 'W4', 'W5', 'W6']\n",
    "            maginput05 = {}\n",
    "            maginput = {}\n",
    "\n",
    "            X = {}\n",
    "            a_idx = np.where(tstmp >= attData['time'])[0][-1]\n",
    "            X['dateTime'] = tstmp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "            X['x1'] = attData['Altitude'][a_idx]\n",
    "            X['x2'] = attData['GEO_Lat'][a_idx]\n",
    "            X['x3'] = attData['GEO_Long'][a_idx]\n",
    "            alpha0 = attData['Pitch'][a_idx]\n",
    "            L = attData['L_Shell'][a_idx]\n",
    "\n",
    "            #T05 model\n",
    "            model05 = MagFields(options = [0,0,0,0,0], kext = 11)\n",
    "            for i in T05Keys:\n",
    "                maginput05[i] = float(omniData[i][idx])\n",
    "            try: \n",
    "                Tb = model05.bounce_period(X, maginput05, KE)\n",
    "            except:\n",
    "                Tb = 0\n",
    "            return Tb, X, L, alpha0\n",
    "        \n",
    "        if len(ddloc) >= 2:\n",
    "            # ref_period = TbSL\n",
    "            att_idx = np.where(t[so[i]] >= a['time'])[0][-1]\n",
    "            ref_period = Tsl(a['L_Shell'][att_idx], a['Pitch'][att_idx], c*beta(KE))\n",
    "            threshold = 0.1*ref_period # lowest allowable difference between two change-in-periods\n",
    "            \n",
    "            # Need to make this work for half periods as well\n",
    "            indices = np.where(np.convolve(np.abs(ddloc) < threshold, \\\n",
    "                                           np.ones(2), mode='valid') == 2)[0]\n",
    "            if len(indices) == 1: # if there is exactly one microburst\n",
    "                tstmp_start = loc[indices[0]] - pd.Timedelta(seconds=0.2)\n",
    "                tstmp_final = loc[indices[0]+3] + pd.Timedelta(seconds=0.2)\n",
    "                et.extend(np.where(t == tstmp_final)[0])\n",
    "                st.extend(np.where(t == tstmp_start)[0])\n",
    "            elif len(ddloc) >= 4 and len(indices) >= 1: # splits up sequences\n",
    "                for j in range(len(indices)-1):\n",
    "                    # if you are looking at the first index and there is not a jump, that is a start time\n",
    "                    if j == 0 and indices[j + 1] - indices[j] == 1:\n",
    "                        tstmp_start = loc[indices[j]] - pd.Timedelta(seconds=0.2)\n",
    "                        st.extend(np.where(t == tstmp_start)[0])\n",
    "                    elif j == 1: # otherwise, nothing happens!\n",
    "                        pass\n",
    "                    # if previously, you were not in a consecutive streak, but now you are, that is a start time\n",
    "                    elif indices[j+1] - indices[j] == 1 and indices[j] - indices[j-1] > 1:\n",
    "                        tstmp_start = loc[indices[j]] - pd.Timedelta(seconds=0.2)\n",
    "                        st.extend(np.where(t == tstmp_start)[0])\n",
    "                    # if previously, you were in a consecutive streak, but now you are not, that is an endtime\n",
    "                    elif indices[j+1] - indices[j] > 1 and indices(j) - indices[j-1] == 1:\n",
    "                        tstmp_final = loc[indices[j]+3] + pd.Timedelta(seconds=0.2)\n",
    "                        et.extend(np.where(t == tstmp_final)[0])\n",
    "\n",
    "                # end condition if the previous one wasn't met\n",
    "                if len(st) > len(et):\n",
    "                    tstmp_final = loc[indices[j]+3] + pd.Timedelta(seconds=0.2)\n",
    "                    et.extend(np.where(t == tstmp_final)[0])\n",
    "    \n",
    "    # The variables st and et contain all intervals that satisfy the strict bounce period condition.\n",
    "    # Now we will check the other conditions.\n",
    "    st = np.sort(st)\n",
    "    et = np.sort(et)\n",
    "\n",
    "    final_st = []\n",
    "    final_et = []\n",
    "    finalpksi = []\n",
    "\n",
    "    for k in range(len(st)):\n",
    "        [Tb, current_X, current_L, current_alpha0] = tsyganenko05(a, t[st[k]]) \n",
    "#         if Tb == 0:\n",
    "#             Tb = Tsl\n",
    "        \n",
    "        ok_range_for_this_index = np.arange(st[k], et[k]+1).tolist()\n",
    "        these_pk_indices = [index for index, value in enumerate(pksi) if value in ok_range_for_this_index]\n",
    "        relative_pks = [pksi[x]-st[k] for x in these_pk_indices]\n",
    "        current_per = np.mean(np.diff(relative_pks))*0.02 # mean period\n",
    "        \n",
    "        okper = np.abs(current_per - Tb) < 0.15*Tb or np.abs(current_per - Tb/2) < 0.075*Tb\n",
    "        \n",
    "        interval = rate[st[k]:et[k]]\n",
    "        \n",
    "        widths = peak_widths(interval, relative_pks, rel_height=0.5)\n",
    "        \n",
    "        lowvarw = np.std(widths[0]) <= 0.4*np.mean(widths[0])\n",
    "        \n",
    "        # Needed to make the condition \"isol\"\n",
    "        desired_starttime = t[st[k]] - pd.Timedelta(seconds=3.5)\n",
    "        idxs = np.abs(np.array(t, dtype='datetime64') - np.datetime64(desired_starttime)).argmin()\n",
    "        interval2_start = idxs\n",
    "\n",
    "        desired_endtime = t[et[k]] + pd.Timedelta(seconds=3.5)\n",
    "        idxe = np.abs(np.array(t, dtype='datetime64') - np.datetime64(desired_endtime)).argmin()\n",
    "        interval2_end = idxe\n",
    "        interval2 = rate[interval2_start:interval2_end]\n",
    "\n",
    "        interval = rate[st[k]:et[k]]\n",
    "        maxdis = max(interval) - min(interval)\n",
    "        [loc2, _] = find_peaks(interval2, prominence=0.1*maxdis, distance=3)\n",
    "        intermediate_list = []\n",
    "        intermediate_list = (st[k]-175 + loc2).tolist()\n",
    "        all_pks_for_interval = [pksi[x] for x in these_pk_indices]\n",
    "        for item in all_pks_for_interval:\n",
    "            if item in intermediate_list:\n",
    "                intermediate_list.remove(item)\n",
    "\n",
    "        isol = len(intermediate_list) < 10\n",
    "        unq = len(np.unique(rate[st[k]:et[k]])) > 10\n",
    "        \n",
    "        passesAllConditions = isol and unq and lowvarw and okper\n",
    "\n",
    "        if passesAllConditions == True:\n",
    "            if len(st) > 1:\n",
    "                per.append(current_per)\n",
    "                L.append(current_L)\n",
    "                alpha0.append(current_alpha0)\n",
    "                X.append(current_X)\n",
    "                final_st.append(st[k])\n",
    "                final_et.append(et[k])\n",
    "                finalpksi.append(all_pks_for_interval)\n",
    "                \n",
    "            else:\n",
    "                final_st.extend(st)\n",
    "                final_et.extend(et)\n",
    "                finalpksi.append(all_pks_for_interval)\n",
    "    \n",
    "    data = pd.DataFrame({'t': [h.times[x] for x in final_st], 'per': per, 'L': L, 'alpha0': alpha0})\n",
    "    return final_st, final_et, finalpksi, data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the ChorusWaves search script.\n",
      "Enter a start date formatted as YYYY, MM, DD, or press r for the most recently queried dates: r\n",
      "Collecting data for January 29, 2000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\sampex\\load.py:179: UserWarning: The SAMPEX HILT data is not in order for 2000029.\n",
      "  warnings.warn(f\"The SAMPEX HILT data is not in order for {self.load_date_str}.\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the ChorusWaves search script.\")\n",
    "\n",
    "if 'first_day' in globals():\n",
    "    fdo = first_day\n",
    "    ldo = last_day\n",
    "    first_day = input(\"Enter a start date formatted as YYYY, MM, DD, or press r for the most recently queried dates: \")\n",
    "else:\n",
    "    first_day = input(\"Enter a start date formatted as YYYY, MM, DD: \")\n",
    "\n",
    "if first_day == \"r\":\n",
    "    try:\n",
    "        first_day = fdo\n",
    "        last_day = ldo\n",
    "    except:\n",
    "        print(\"Looks like you cleared your old dates; can't use that command, sorry.\")\n",
    "else:\n",
    "    last_day = input(\"Enter an end date, or press s to look at one day: \")\n",
    "    if last_day == \"s\":\n",
    "        last_day = first_day\n",
    "        first_day = first_day.split(\", \")\n",
    "        last_day = last_day.split(\", \")\n",
    "        first_day_d = datetime(int(first_day[0]), int(first_day[1]), int(first_day[2]))\n",
    "        last_day_d = datetime(int(last_day[0]), int(last_day[1]), int(last_day[2]))\n",
    "    else:\n",
    "        first_day = first_day.split(\", \")\n",
    "        last_day = last_day.split(\", \")\n",
    "        first_day_d = datetime(int(first_day[0]), int(first_day[1]), int(first_day[2]))\n",
    "        last_day_d = datetime(int(last_day[0]), int(last_day[1]), int(last_day[2]))\n",
    "\n",
    "date_list = [first_day_d + timedelta(days=i) for i in range((last_day_d - first_day_d).days + 1)]\n",
    "\n",
    "for day in date_list:\n",
    "    formatted_date = day.strftime(\"%B %d, %Y\")\n",
    "    print(f\"Collecting data for {formatted_date} ...\")\n",
    "    h = sampex.HILT(day) # count rate data (thanks Mike!)\n",
    "    h.load()\n",
    "    a = sampex.Attitude(day) # attitude data (thanks Mike!)\n",
    "    a.load()\n",
    "\n",
    "    print(\"Running O'Brien algorithm...\")\n",
    "    d = obrien(h) # call O'Brien function\n",
    "\n",
    "    so = d['st']\n",
    "    eo = d['et']\n",
    "    ns = d['ns']\n",
    "\n",
    "    print(\"Searching for bouncing packets...\")\n",
    "#     print(\"There are\", len(so), \"intervals to check.\")\n",
    "#     if len(so) < 10:\n",
    "#         print(\"Pretty quiet day today! This should run (relatively) quickly.\")\n",
    "#     else:\n",
    "#         print(\"This might take a while... hope you have an activity to do...\")\n",
    "    [st, et, pksi, data] = bouncingPackets(so, eo, h, a) \n",
    "    print(len(st), \"intervals were found. Plotting now.\")\n",
    "\n",
    "    for j in range(len(st)):\n",
    "        plt.figure(j)\n",
    "        plt.plot(h.times[st[j]:et[j]], h.counts[st[j]:et[j]])\n",
    "        plt.plot(h.times[pksi[j]], h.counts[pksi[j]], marker='d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
