{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "## Bouncing Packet Locator Script\n",
    "\n",
    "# Date Created: 05/15/23 (Original Script, in MATLAB)\n",
    "# Date Created: 01/03/24 (This Script)\n",
    "# Last Modified: 4/30/24\n",
    "\n",
    "# Author: Max Feinland for Blum Research Group, LASP\n",
    "\n",
    "# Inputs: start date, end date\n",
    "\n",
    "# Description: complete driver script. The user will be prompted to specify the dates \n",
    "# they would like to see. This script will then print out and plot all of the \n",
    "# microbursts found in the specified time period.\n",
    "\n",
    "# housekeeping\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacepy.datamodel\n",
    "import spacepy.time as spt\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil.parser\n",
    "from IRBEM import MagFields\n",
    "import sampex\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.dates as dates\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "import copy\n",
    "import time\n",
    "import stopit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## O'Brien\n",
    "\n",
    "# Date created: 3/21/23 (in MATLAB)\n",
    "# or 01/04/24 in Python\n",
    "# Last modified: 02/04/24 (to fix a STUPID indexing issue, smh)\n",
    "# Author: Max Feinland for Blum Research Group, LASP\n",
    "\n",
    "# Inputs: sampex package HILT object\n",
    "\n",
    "# Outputs: starting & ending time of each microburst, plus time intervals\n",
    "\n",
    "def obrien(data):\n",
    "    N20 = data['counts'] # count rate sampled every 20 ms\n",
    "    time20 = data['time'] # time every 20 ms\n",
    "\n",
    "    # I'm sure there's a more efficient way to do this, but I don't know it\n",
    "    df = pd.DataFrame({'time': data['time'], 'counts': data['counts']})\n",
    "\n",
    "    df.set_index('time', inplace=True) # set time column as the index\n",
    "\n",
    "    # resample the dataframe to 100 ms intervals and sum the counts in each interval\n",
    "    N100 = df.resample('100ms').sum()\n",
    "\n",
    "    A500 = N100.rolling(5, center=True).mean() # 5-observation centered rolling mean (over 500 ms)\n",
    "\n",
    "    condition = np.divide((N100.counts - A500.counts), np.sqrt(1 + A500.counts)) # O'Brien et al 2003\n",
    "    \n",
    "    ns = np.argwhere(condition > 10)\n",
    "    ns = [item[0] for item in ns]\n",
    "\n",
    "    epsilon = 10; # if two flagged indices are spaced less than this distance apart, \n",
    "    # they are probably part of the same microburst\n",
    "\n",
    "    # initializing\n",
    "    starts = []\n",
    "    ends = []\n",
    "\n",
    "    dn = np.diff(ns) # difference in time between instances of the condition being true\n",
    "\n",
    "    # finding extended periods of the condition being true\n",
    "    for i in np.arange(1,len(dn)-10):\n",
    "        if dn[i] < epsilon and dn[i+1] < epsilon and dn[i-1] >= epsilon: # start condition\n",
    "            starts.append(ns[i])\n",
    "            for j in np.arange(i+1, len(dn)-1):\n",
    "                if dn[j] < epsilon and dn[j+1] >= epsilon:\n",
    "                    ends.append(ns[j]) # end condition\n",
    "                    break\n",
    "        elif dn[i] <= epsilon and i == 1: # start condition (edge case)\n",
    "            starts.append(ns[i])\n",
    "            for j in np.arange(i+1, len(dn-1)):\n",
    "                if dn[j] <= epsilon:\n",
    "                    ends.append(ns[j])\n",
    "                    break\n",
    "        elif i == len(dn): # end condition (edge case)\n",
    "            ends.append(ns[i])\n",
    "\n",
    "    if len(starts) > len(ends):\n",
    "        ends.append(starts[len(starts)-1] + 10)\n",
    "        \n",
    "    starts = [x - 2 for x in starts] # pad with 0.2 seconds \n",
    "    ends = [x + 10 for x in ends] # pad with 1 second\n",
    "\n",
    "\n",
    "    def changeCadence(time20, times100):\n",
    "        # times100 is the list of timestamps for whatever it is: starts, ends, ns\n",
    "        # times20 is the entire list of time20 for the day in question\n",
    "        try:\n",
    "            # try list comprehension first, it's faster\n",
    "            idx20 = [time20.get_loc(tstmp) for tstmp in times100] \n",
    "        except: \n",
    "            idx20 = [] # clear and start over\n",
    "            idx20 = [np.abs((time20 - target).values).argmin() for target in times100] \n",
    "            # if a timestamp is missing from time20,\n",
    "            # you will have to find the minimum timestamp (that is closest in time)\n",
    "        return idx20\n",
    "\n",
    "    # reverting indices to 20 ms cadence\n",
    "\n",
    "    starts20 = changeCadence(time20, N100.index[starts])\n",
    "    ends20 = changeCadence(time20, N100.index[ends])\n",
    "    ns20 = changeCadence(time20, N100.index[ns])\n",
    "\n",
    "    # yay, your output is ready!\n",
    "    d = dict(); \n",
    "    d['st'] = starts20\n",
    "    d['et'] = ends20\n",
    "    d['ns'] = ns20\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsyganenko05(tstmp):\n",
    "    \n",
    "    # Author: Max Feinland, although Mike Shumko gets a lot of credit for the skeleton\n",
    "    # Purpose: Calculates bounce period from T05 model.\n",
    "    \n",
    "    # If all of the preliminary conditions checked are true, the T05 model\n",
    "    # will be called to check if the bounce period is within tolerance.\n",
    "    \n",
    "    # Also pulls attitude data. This saves some time, since you won't need to download\n",
    "    # the attitude data otherwise.\n",
    "    \n",
    "    a = sampex.Attitude(tstmp) # attitude data (thanks Mike!)\n",
    "    a.load()\n",
    "    # change this for your needs. Of course, this pathing is for my machine.\n",
    "    omniLoc = 'C:/Users/maxim/.spacepy/data/omnidata.h5'\n",
    "    omniData = spacepy.datamodel.fromHDF5(omniLoc)\n",
    "    omniT = np.array([dateutil.parser.parse(i.decode()) for i in omniData['UTC']])\n",
    "    idx = np.where(tstmp >= omniT)[0][-1] # find relevant omni index\n",
    "    T05Keys = ['Dst', 'Pdyn', 'ByIMF', 'BzIMF', 'W1', 'W2', 'W3', 'W4', 'W5', 'W6']\n",
    "    maginput05 = {}\n",
    "    maginput = {}\n",
    "\n",
    "    X = {}\n",
    "    # find relevant attitude index\n",
    "    a_idx = np.where(tstmp >= a['time'])[0][-1]\n",
    "    # X is the time & location of the particle (lla)\n",
    "    X['dateTime'] = tstmp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    X['x1'] = a['Altitude'][a_idx]\n",
    "    X['x2'] = a['GEO_Lat'][a_idx]\n",
    "    X['x3'] = a['GEO_Long'][a_idx]\n",
    "    # These are returned for data processing reasons (later on). Not necessary for the model\n",
    "    L = a['L_Shell'][a_idx]\n",
    "    MLT = a['MLT'][a_idx]\n",
    "    \n",
    "    KE = 1000 # keV, true of all particles observed at SAMPEX\n",
    "\n",
    "    #T05 model\n",
    "    model05 = MagFields(options = [0,0,0,0,0], kext = 11)\n",
    "    for i in T05Keys:\n",
    "        maginput05[i] = float(omniData[i][idx])\n",
    "    Tb = model05.bounce_period(X, maginput05, KE) # call model\n",
    "    return Tb, X, L, MLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bouncingPackets(so, eo, h):\n",
    "    # Date created: not sure. Sort of a Ship of Theseus thing. I originally created this in MATLAB\n",
    "    # 04/25/23, but have no idea when it was created in Python, unfortunately. Likely Jan 2024.\n",
    "    # Last modified: 4/30/24\n",
    "    # Purpose: takes intervals specified from the O'Brien algorithm and searches for consistently-spaced,\n",
    "    # low-width-variance, decreasing, isolated peaks whose bounce periods \n",
    "    # match up with the expected values.\n",
    "    \n",
    "    \n",
    "    # Intializing variables\n",
    "    pksi = []\n",
    "    st = []\n",
    "    et = []\n",
    "    \n",
    "    per = []\n",
    "    L = []\n",
    "    X = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    MLT = []\n",
    "    TbT05 = []\n",
    "    tb_check = []\n",
    "    \n",
    "    # load in HILT data\n",
    "    t = h.times\n",
    "    rate = h.counts\n",
    "    # the \"with\" statement prevents excessive runtimes, at the expense of a couple of days' worth of data\n",
    "    with stopit.ThreadingTimeout(60) as context_manager:\n",
    "        for i in range(len(so)):\n",
    "\n",
    "            interval = rate[so[i]:eo[i]] # define just one chunk of the rate data, taken from O'Brien\n",
    "            maxdis = max(interval) - min(interval) # use to generate prominence requirement\n",
    "\n",
    "            # finding peaks with a high enough prominence\n",
    "            [pks, _] = find_peaks(interval, prominence=0.25*maxdis, distance=3)\n",
    "            npks = list(so[i] + loc for loc in pks)\n",
    "            pksi.extend(npks) # add to \"pksi\" list, to access later (plotting)\n",
    "\n",
    "            loc = pd.to_datetime(t[npks])\n",
    "\n",
    "            dloc = [loc[i + 1] - loc[i] for i in range(len(loc) - 1)] # peak spacing\n",
    "            dloc = [x.total_seconds() for x in dloc]\n",
    "            ddloc = np.diff(dloc) # change in peak spacing; must be sufficiently small, i.e.\n",
    "            # the peaks must be very regularly spaced.\n",
    "\n",
    "            if len(ddloc) >= 2:\n",
    "                threshold = 0.05 # lowest allowable change in peak spacing/period\n",
    "\n",
    "                # find a run of at least 2 indices that meet this criterion. That means 4 peaks\n",
    "                indices = np.where(np.convolve(np.abs(ddloc) < threshold, \\\n",
    "                                               np.ones(2), mode='valid') == 2)[0]\n",
    "                if len(indices) == 1: # if there is exactly one microburst\n",
    "                    tstmp_start = loc[indices[0]] - pd.Timedelta(seconds=0.2)\n",
    "                    tstmp_final = loc[indices[0]+3] + pd.Timedelta(seconds=0.2)\n",
    "                    et.extend(np.where(t == tstmp_final)[0])\n",
    "                    st.extend(np.where(t == tstmp_start)[0])\n",
    "                elif len(indices) > 1:\n",
    "                    for j in range(len(indices)-1):\n",
    "                        # if you are looking at the first index and there is not a jump, that is a start time\n",
    "                        if j == 0 and indices[j + 1] - indices[j] == 1:\n",
    "                            tstmp_start = loc[indices[j]] - pd.Timedelta(seconds=0.2)\n",
    "                            st.extend(np.where(t == tstmp_start)[0])\n",
    "                        elif j == 0: # otherwise, nothing happens!\n",
    "                            pass\n",
    "                        # if previously, you were not in a consecutive streak, \n",
    "                        # but now you are, that is a start time\n",
    "                        elif indices[j+1] - indices[j] == 1 and indices[j] - indices[j-1] > 1:\n",
    "                            tstmp_start = loc[indices[j]] - pd.Timedelta(seconds=0.2)\n",
    "                            st.extend(np.where(t == tstmp_start)[0])\n",
    "                        # if previously, you were in a consecutive streak, but now you are not,\n",
    "                        # that is an endtime\n",
    "                        elif indices[j+1] - indices[j] > 1 and indices[j] - indices[j-1] == 1:\n",
    "                            tstmp_final = loc[indices[j]+3] + pd.Timedelta(seconds=0.2)\n",
    "                            et.extend(np.where(t == tstmp_final)[0])\n",
    "\n",
    "                    # end condition if the previous one wasn't met\n",
    "                    if len(st) > len(et):\n",
    "                        tstmp_final = loc[indices[j]+3] + pd.Timedelta(seconds=0.2)\n",
    "                        et.extend(np.where(t == tstmp_final)[0])\n",
    "\n",
    "    # The variables st and et contain all intervals that satisfy the strict bounce period condition.\n",
    "    # Now we will check the other conditions.\n",
    "        \n",
    "    # Check if timed out\n",
    "    if context_manager.state == context_manager.EXECUTED:\n",
    "        st = np.sort(st)\n",
    "        et = np.sort(et)\n",
    "    \n",
    "    elif context_manager.state == context_manager.TIMED_OUT:\n",
    "        st = []\n",
    "        et = []\n",
    "        print(\"Timed out execution. Skipping this day.\")\n",
    "\n",
    "    final_st = []\n",
    "    final_et = []\n",
    "    finalpksi = []\n",
    "    dec = []\n",
    "    MLT = []\n",
    "    L = []\n",
    "    TbT05 = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "\n",
    "    for k in range(len(st)):\n",
    "        # Find all of the indices in the rate data corresponding to this interval.\n",
    "        ok_range_for_this_index = np.arange(st[k], et[k]+1).tolist()\n",
    "        # Find all the values in pksi that are contained within these indices.\n",
    "        these_pk_indices = [index for index, value in enumerate(pksi) if value in ok_range_for_this_index]\n",
    "        relative_pks = [pksi[x]-st[k] for x in these_pk_indices]\n",
    "        current_per = np.mean(np.diff(relative_pks))*0.02 # mean period\n",
    "        \n",
    "        interval = rate[st[k]:et[k]]\n",
    "        unq = len(np.unique(interval)) > 5\n",
    "        if unq == True:\n",
    "            try:\n",
    "                widths = peak_widths(interval, relative_pks, rel_height=0.5)\n",
    "                incrw = np.all(np.diff(widths[0]) >= -3)\n",
    "            except:\n",
    "                # sometimes peak_widths errors out :(\n",
    "                incrw = False\n",
    "\n",
    "            # Needed to make the condition \"isol\"\n",
    "            # Find the index of the time that is 3 seconds before the identified starttime\n",
    "            desired_starttime = t[st[k]] - pd.Timedelta(seconds=3)\n",
    "            idxs = np.abs(np.array(t, dtype='datetime64') - np.datetime64(desired_starttime)).argmin()\n",
    "            interval2_start = idxs\n",
    "\n",
    "            # Find the index of the time that is 3 seconds after the identified endtime\n",
    "            desired_endtime = t[et[k]] + pd.Timedelta(seconds=3)\n",
    "            idxe = np.abs(np.array(t, dtype='datetime64') - np.datetime64(desired_endtime)).argmin()\n",
    "            interval2_end = idxe\n",
    "            interval2 = rate[interval2_start:interval2_end]\n",
    "\n",
    "            interval = rate[st[k]:et[k]]\n",
    "            maxdis = max(interval) - min(interval)\n",
    "            \n",
    "            # Find all peaks within the new, longer interval that satisfy the prominence requirement\n",
    "            [loc2, _] = find_peaks(interval2, prominence=0.25*maxdis, distance=3)\n",
    "            intermediate_list = []\n",
    "            intermediate_list = (interval2_start + loc2).tolist()\n",
    "            all_pks_for_interval = [pksi[x] for x in these_pk_indices]\n",
    "            for item in all_pks_for_interval:\n",
    "                if item in intermediate_list:\n",
    "                    intermediate_list.remove(item)\n",
    "            isol = len(intermediate_list) < 12\n",
    "                \n",
    "            # this is for the decreasing interpolation\n",
    "            timestamps = t[st[k]:et[k]]\n",
    "            timestamps = np.array(timestamps, dtype='datetime64')\n",
    "            timedelta_index = pd.to_timedelta(timestamps - timestamps.astype('datetime64[D]'))\n",
    "            intt = timedelta_index.total_seconds()\n",
    "\n",
    "            peaks, props = find_peaks(-interval, prominence=0.15 * np.ptp(interval))\n",
    "            peaks = np.concatenate(([0], peaks, [len(interval) - 1]))\n",
    "\n",
    "            vq = np.interp(intt, [intt[x] for x in peaks], [interval[x] for x in peaks])\n",
    "            adj = interval - vq + np.min(interval)\n",
    "            \n",
    "            # sometimes there's a funny edge condition that outputs the out-of-bound indices as peaks.\n",
    "            # so this if statement is just taking care of that\n",
    "            if len(adj) in relative_pks:\n",
    "                relative_pks.remove(len(adj))\n",
    "                \n",
    "            max_prom = max(props[\"prominences\"])\n",
    "            # all peaks, except the first one, have to decrease (or not increase by a whole lot)\n",
    "            decr = np.all(np.diff(adj[relative_pks[1:]]) < 0.2*max_prom)\n",
    "\n",
    "            passesMostConditions = isol and unq and incrw and decr\n",
    "        else:\n",
    "            passesMostConditions = False\n",
    "\n",
    "\n",
    "        # This is split up into two cases because the TbT05 calculation takes FOREVER to run.\n",
    "        # So, the first three conditions must be true before the last one will be checked.\n",
    "        if passesMostConditions == True:\n",
    "            print('Calling Tsyganenko 2005 model. This may take a while; please be patient.')\n",
    "            try:\n",
    "                [Tb, current_X, current_L, current_MLT] = tsyganenko05(t[st[k]])\n",
    "            except ValueError:\n",
    "                Tb = None\n",
    "                print(\"There was a problem calling the model, and this interval was not used.\")\n",
    "\n",
    "            if Tb is not None:\n",
    "                per.append(current_per)\n",
    "                L.append(current_L)\n",
    "                MLT.append(current_MLT)\n",
    "                TbT05.append(Tb)\n",
    "                final_st.append(st[k])\n",
    "                final_et.append(et[k])\n",
    "                finalpksi.append(all_pks_for_interval)\n",
    "                lat.append(current_X['x2'])\n",
    "                lon.append(current_X['x3'])\n",
    "            # dec.append(decr)\n",
    "    \n",
    "    data = pd.DataFrame({'t': [h.times[x] for x in final_st], 'per': per, 'tb': TbT05, 'L': L, \n",
    "                        'MLT': MLT, 'lat': lat, 'lon': lon})\n",
    "    data = data.round({'per': 4, 'tb': 4, 'L': 3, 'MLT': 3, 'lat': 3, 'lon': 3})\n",
    "    # so the dataframe isn't long af\n",
    "    return final_st, final_et, finalpksi, data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the ChorusWaves search script.\n",
      "Coded by Max Feinland for Blum Research Group, 2023-2024.\n",
      "The most recently queried dates were January 01, 2001 to January 30, 2001.\n",
      "* Press o to set the first date as 1 day after the last date of the last query.\n",
      "* Press r to rerun the most recently queried dates.\n",
      "Otherwise, enter a start date formatted as \"YYYY, MM, DD\": o\n",
      "Enter an end date, or enter the number of days as an integer (from 1 to 30): 30\n",
      "\n",
      "Collecting data for January 31, 2001...\n"
     ]
    }
   ],
   "source": [
    "# The \"main\" script, if you will.\n",
    "# Prompts the user for dates, with a number of options (mostly so my life is easier.)\n",
    "# Runs O'Brien, then bouncingPackets. Plots the results and saves to a dataframe, which is then\n",
    "# saved to a file.\n",
    "# Date created: 01/04/2024\n",
    "# Last modified: 04/30/2024\n",
    "# Author: Max Feinland\n",
    "\n",
    "print(\"Welcome to the ChorusWaves search script.\", flush=True)\n",
    "print(\"Coded by Max Feinland for Blum Research Group, 2023-2024.\", flush=True)\n",
    "\n",
    "print_yn = 1\n",
    "hilt_already_loaded = None\n",
    "## This is all for user input.\n",
    "\n",
    "if 'first_day_d' in globals():\n",
    "    fdo = first_day_d\n",
    "    ldo = last_day_d\n",
    "    f_fd = fdo.strftime(\"%B %d, %Y\")\n",
    "    f_ld = ldo.strftime(\"%B %d, %Y\")\n",
    "    if f_fd == f_ld:\n",
    "        print(f\"The most recently queried date was {f_fd}.\", flush=True)\n",
    "    else:\n",
    "        print(f\"The most recently queried dates were {f_fd} to {f_ld}.\", flush=True)\n",
    "        \n",
    "    print(\"* Press o to set the first date as 1 day after the last date of the last query.\", flush=True)\n",
    "    print(\"* Press r to rerun the most recently queried dates.\", flush=True)\n",
    "    first_day = input(\"Otherwise, enter a start date formatted as \\\"YYYY, MM, DD\\\": \")\n",
    "else:\n",
    "    first_day = input(\"Enter a start date formatted as \\\"YYYY, MM, DD\\\": \" )\n",
    "\n",
    "if first_day == \"r\":\n",
    "    try:\n",
    "        first_day_d = fdo\n",
    "        last_day_d = ldo\n",
    "        if fdo == ldo:\n",
    "            hilt_already_loaded = True # if you keep running the same day over and over, like I do, this saves time\n",
    "    except:\n",
    "        print(\"Looks like you cleared your old dates; can't use that command, sorry.\")\n",
    "elif first_day == \"o\":\n",
    "    first_day_d = ldo + timedelta(days=1)\n",
    "    last_day = input(\"Enter an end date, or enter the number of days as an integer (from 1 to 30): \")\n",
    "    try:\n",
    "        last_day = int(last_day)\n",
    "        if last_day in np.arange(1, 31):\n",
    "            last_day_d = first_day_d + timedelta(days = last_day - 1)\n",
    "        else:\n",
    "            print(\"Number of days not allowed. Please try again.\")\n",
    "            print(\"Zero days is not allowed for obvious reasons, and more than 30 are not recommended due to processing time.\")\n",
    "    except:\n",
    "        try:\n",
    "            last_day = last_day.split(\", \")\n",
    "            last_day_d = datetime(int(last_day[0]), int(last_day[1]), int(last_day[2]))\n",
    "        except:\n",
    "            print(\"Unrecognized command; could not execute.\")\n",
    "else:\n",
    "    first_day = first_day.split(\", \")\n",
    "    first_day_d = datetime(int(first_day[0]), int(first_day[1]), int(first_day[2]))\n",
    "    last_day = input(\"Enter an end date, or enter the number of days as an integer (from 1 to 30): \")\n",
    "    try:\n",
    "        last_day = int(last_day)\n",
    "        if last_day != 0:\n",
    "            last_day_d = first_day_d + timedelta(days = last_day - 1)\n",
    "        else:\n",
    "            print(\"Zero days is not allowed for obvious reasons.\")\n",
    "    except:\n",
    "        try:\n",
    "            last_day = last_day.split(\", \")\n",
    "            last_day_d = datetime(int(last_day[0]), int(last_day[1]), int(last_day[2]))\n",
    "        except:\n",
    "            print(\"Unrecognized command; could not execute.\")\n",
    "date_list = [first_day_d + timedelta(days=i) for i in range((last_day_d - first_day_d).days + 1)]\n",
    "\n",
    "\n",
    "# Now we are getting to the the function calls, etc\n",
    "\n",
    "save_counter = 0\n",
    "data = pd.DataFrame()\n",
    "for day in date_list:\n",
    "    formatted_date = day.strftime(\"%B %d, %Y\")\n",
    "\n",
    "    if print_yn == 1:\n",
    "        print(f\"\\nCollecting data for {formatted_date}...\")\n",
    "\n",
    "    if hilt_already_loaded == True:\n",
    "        pass\n",
    "    else:\n",
    "        h = sampex.HILT(day) # count rate data (thanks Mike!)\n",
    "        h.load()\n",
    "\n",
    "    if print_yn == 1:\n",
    "        print(\"Running O'Brien algorithm...\")\n",
    "\n",
    "    d = obrien(h) # call O'Brien function\n",
    "\n",
    "    so = d['st']\n",
    "    eo = d['et']\n",
    "    ns = d['ns']\n",
    "\n",
    "    if print_yn == 1:\n",
    "        print(\"Searching for bouncing packets...\")\n",
    "\n",
    "    [stt, ett, pksit, this_data] = bouncingPackets(so, eo, h)\n",
    "    data = pd.concat([data, this_data], ignore_index=True)\n",
    "    filename = \"events_runonapr29.csv\"\n",
    "\n",
    "\n",
    "    if print_yn == 1:\n",
    "        if len(stt) == 0:\n",
    "            print(\"No intervals found.\")\n",
    "        elif len(stt) == 1:\n",
    "            print(\"1 interval found. Plotting now.\")\n",
    "        else:\n",
    "            print(len(stt), \"intervals were found. Plotting now.\")\n",
    "\n",
    "    for j in range(len(stt)):\n",
    "        fig = plt.figure(j)\n",
    "        ax = fig.add_subplot()\n",
    "        plt.grid(True)\n",
    "\n",
    "        ax.plot(h.times[stt[j]-50:ett[j]+75], h.counts[stt[j]-50:ett[j]+75])\n",
    "        ax.plot(h.times[pksit[j]], h.counts[pksit[j]], marker='d')\n",
    "        ax.xaxis.set_major_formatter(dates.DateFormatter('%H:%M:%S.%f')) \n",
    "        ax.set_xlabel('Time (UTC)')\n",
    "        ax.set_ylabel('Counts (#/20 ms)')\n",
    "        ax.set_title('Bouncing Packets on ' + h.times[stt[j]].strftime('%Y-%m-%d'))\n",
    "#         ax.xaxis.label.set_color('white')\n",
    "#         ax.yaxis.label.set_color('white')\n",
    "#         ax.tick_params(colors='white', which='both')\n",
    "#         ax.title.set_color('white')\n",
    "#         ax.title.set_color('white')\n",
    "        plt.show()\n",
    "        if len(stt) == 1:\n",
    "            plt_name = h.times[stt[j]].strftime('%Y%m%d') + '.jpg'\n",
    "        else:\n",
    "            plt_name = h.times[stt[j]].strftime('%Y%m%d') + \"_\" + str(j+1) + '.jpg'\n",
    "        fig.savefig(plt_name)\n",
    "import os.path\n",
    "if os.path.isfile('./' + filename):\n",
    "    data.to_csv(filename, mode='a', sep=',', index=False, header=False, encoding='utf-8')\n",
    "else:\n",
    "    data.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "print('Script complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
